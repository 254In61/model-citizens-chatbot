# ğŸ“Œ AKA Studio Design Chatbot â€” Local LLM Only

This project is a **Client Requirement Collection Chatbot** for AKA Studio.
It works fully offline with your own **TinyLlama local LLM**.

---

## ğŸš€ Whatâ€™s Inside?

âœ… **Flask backend** (`app.py`): Matches client input to your design dataset, generates a friendly suggestion using your own LLM.  
âœ… **FastAPI wrapper** (`main.py`): Wraps your TinyLlama running in Ollama.  
âœ… **Frontend pages**: `index.html`, `chat.html`, `summary.html`, `thankyou.html`.  
âœ… **Dataset**: `aka_studio_designs.xlsx`.

---

## âœ… 1ï¸âƒ£ Requirements

- Python 3.9+
- pip
- ~3GB RAM
- [Ollama installed](https://ollama.com/download)

---

## âœ… 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## âœ… 3ï¸âƒ£ Pull TinyLlama

```bash
ollama pull tinyllama
```

---

## âœ… 4ï¸âƒ£ Run TinyLlama

```bash
ollama run tinyllama
```

---

## âœ… 5ï¸âƒ£ Start FastAPI wrapper

```bash
uvicorn main:app --reload --port 8000
```

---

## âœ… 6ï¸âƒ£ Run Flask server

```bash
python app.py
```

Visit [http://127.0.0.1:5000](http://127.0.0.1:5000)

âœ… **No Gemini needed â€” fully offline, local LLM-powered!**
